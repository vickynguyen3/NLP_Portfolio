{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "A database of semantic relations from various languages is called WordNet. Synsets (synonym sets) are collections of words that have similar meanings that can be used to determine their definitions, use cases, and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Quang\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Quang\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Quang\n",
      "[nltk_data]    |     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('book')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns\n",
    "I selected a noun as an example and outputted all of its synsets, which will be a list of synsets that are relevant to the select word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dress.n.01'),\n",
       " Synset('attire.n.01'),\n",
       " Synset('apparel.n.01'),\n",
       " Synset('dress.v.01'),\n",
       " Synset('dress.v.02'),\n",
       " Synset('dress.v.03'),\n",
       " Synset('dress.v.04'),\n",
       " Synset('preen.v.03'),\n",
       " Synset('dress.v.06'),\n",
       " Synset('dress.v.07'),\n",
       " Synset('trim.v.06'),\n",
       " Synset('dress.v.09'),\n",
       " Synset('dress.v.10'),\n",
       " Synset('snip.v.02'),\n",
       " Synset('dress.v.12'),\n",
       " Synset('dress.v.13'),\n",
       " Synset('dress.v.14'),\n",
       " Synset('dress.v.15'),\n",
       " Synset('dress.v.16'),\n",
       " Synset('full-dress.s.01'),\n",
       " Synset('dress.s.02')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output all synsets from noun\n",
    "wn.synsets('dress')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now select one of the synset from the list of synsets to extract its definition, usage examples, and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Definitions ----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'clothing in general'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract definition \n",
    "print('---- Definitions ----')\n",
    "wn.synset('apparel.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she was refined in her choice of apparel',\n",
       " 'he always bought his clothes at the same store',\n",
       " 'fastidious about his dress']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract usage examples\n",
    "wn.synset('apparel.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('apparel.n.01.apparel'),\n",
       " Lemma('apparel.n.01.wearing_apparel'),\n",
       " Lemma('apparel.n.01.dress'),\n",
       " Lemma('apparel.n.01.clothes')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract lemmas\n",
    "wn.synset('apparel.n.01').lemmas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to view the entire hierarchy of the selected word and see the hypernyms of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('clothing.n.01')\n",
      "Synset('consumer_goods.n.01')\n",
      "Synset('commodity.n.01')\n",
      "Synset('artifact.n.01')\n",
      "Synset('whole.n.02')\n",
      "Synset('object.n.01')\n",
      "Synset('physical_entity.n.01')\n",
      "Synset('entity.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Traverse up the WordNet hierarchy\n",
    "\n",
    "hy = wn.synset('apparel.n.01').hypernyms()[0]\n",
    "# hierarchy for nouns has 'entity' at the top\n",
    "top = wn.synset('entity.n.01')\n",
    "\n",
    "while hy:\n",
    "    print(hy)\n",
    "    if hy == top:\n",
    "        break\n",
    "    if hy.hypernyms():\n",
    "        hy = hy.hypernyms()[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation, it is apparent that the hierarchy is set up so that each noun is classified as a subclass of its parent. This trend continues until we encounter the 'entity' noun, which encompasses all nouns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the code shows various ways to get hierarchical relations. There will be an empty list when there are no applicable words that exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms:  [Synset('clothing.n.01')]\n",
      "Hyponyms:  [Synset('workwear.n.01')]\n",
      "Meronyms:  [] []\n",
      "Holoynyms:  []\n",
      "Antonyms:  []\n"
     ]
    }
   ],
   "source": [
    "# output hypernymns, hyponyms, meronyms, holonyms, antonyms (or an empty list if none exist)\n",
    "\n",
    "# Hypernyms\n",
    "print('Hypernyms: ', wn.synset('apparel.n.01').hypernyms())\n",
    "\n",
    "# Hyponyms\n",
    "print('Hyponyms: ',  wn.synset('apparel.n.01').hyponyms())\n",
    "\n",
    "# Meronyms\n",
    "print('Meronyms: ', wn.synset('apparel.n.01').part_meronyms(), wn.synset('apparel.n.01').substance_meronyms())\n",
    "\n",
    "# Holonyms\n",
    "print('Holoynyms: ', wn.synset('apparel.n.01').member_holonyms())\n",
    "\n",
    "# Antonyms\n",
    "print('Antonyms: ', wn.synset('apparel.n.01').lemmas()[0].antonyms())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbs\n",
    "Now let's try an example with verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sing.v.01'),\n",
       " Synset('sing.v.02'),\n",
       " Synset('sing.v.03'),\n",
       " Synset('whistle.v.05'),\n",
       " Synset('spill_the_beans.v.01')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output all synsets from verb\n",
    "wn.synsets('sing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the definition, usage examples, and lemmas from the selected synset of the verb variation of the word chosen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'produce tones with the voice'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract definition \n",
    "wn.synset('sing.v.02').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She was singing while she was cooking', 'My brother sings very well']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract usage examples\n",
    "wn.synset('sing.v.02').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('sing.v.02.sing')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract lemmas\n",
    "wn.synset('sing.v.02').lemmas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the code is traversing up the hierarchy of the selected verb and outputting the synsets as it goes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('talk.v.02'),\n",
       " Synset('communicate.v.02'),\n",
       " Synset('interact.v.01'),\n",
       " Synset('act.v.01')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traverse up the WordNet hierarchy\n",
    "sing = wn.synset('sing.v.02')\n",
    "hy = lambda s: s.hypernyms()\n",
    "list(sing.closure(hy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation of the way verbs are organized in WordNet is that there is no common hypernym for all verbs because of the difference between the organization of nouns and verbs. Unlike nouns, each word terminates its hierarchy in different places"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphy\n",
    "morphy() function will return the base form of the selected word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "hug\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# use morphy to find many different forms of the word\n",
    "print(wn.morphy('loving'))\n",
    "print(wn.morphy('hugged', wn.VERB))\n",
    "print(wn.morphy('hugged', wn.ADV))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wu-Palmer Similarity Metric and Lesk Algorithm\n",
    "To assess how closely related two words are in terms of how they are used in a language, a similarity measure is frequently utilized. Usually, the score is assigned between 0 (few similarities) and 1 (identity).\n",
    "\n",
    "Below, I chose two words that I believe are similar to a certain degree to demonstrate Wu-Palmer Similarity Metric, which is based on two words and their most explicit common ancester node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wu-Palmer Similarity Metric\n",
    "wn.wup_similarity(wn.synset('lady.n.01'), wn.synset('female.n.01'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am trying the Lesk Algorithm, which returns the synset with the most overlapped words between a given context phrase and each synset's definitions for the selected word. Additionally, we can give a pos argument for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('hit.n.01') (baseball) a successful stroke in an athletic contest (especially in baseball)\n",
      "Synset('hit.n.02') the act of contacting one thing with another\n",
      "Synset('hit.n.03') a conspicuous success\n",
      "Synset('collision.n.01') (physics) a brief event in which two or more bodies come together\n",
      "Synset('hit.n.05') a dose of a narcotic drug\n",
      "Synset('hit.n.06') a murder carried out by an underworld syndicate\n",
      "Synset('hit.n.07') a connection made via the internet to another website\n",
      "Synset('hit.v.01') cause to move by striking\n",
      "Synset('hit.v.02') hit against; come into sudden contact with\n",
      "Synset('hit.v.03') deal a blow to, either with the hand or with an instrument\n",
      "Synset('reach.v.01') reach a destination, either real or abstract\n",
      "Synset('hit.v.05') affect or afflict suddenly, usually adversely\n",
      "Synset('shoot.v.01') hit with a missile from a weapon\n",
      "Synset('stumble.v.03') encounter by chance\n",
      "Synset('score.v.01') gain points in a game\n",
      "Synset('hit.v.09') cause to experience suddenly\n",
      "Synset('strike.v.04') make a strategic, offensive, assault against an enemy, opponent, or a target\n",
      "Synset('murder.v.01') kill intentionally and with premeditation\n",
      "Synset('hit.v.12') drive something violently into a location\n",
      "Synset('reach.v.02') reach a point in time, or a certain state or level\n",
      "Synset('strike.v.10') produce by manipulating keys or strings of musical instruments, also metaphorically\n",
      "Synset('hit.v.15') consume to excess\n",
      "Synset('hit.v.16') hit the intended target or goal\n",
      "Synset('hit.v.17') pay unsolicited and usually unwanted sexual attention to\n"
     ]
    }
   ],
   "source": [
    "# Lesk Algorithm\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "for ss in wn.synsets('hit'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('hit.n.07')\n",
      "Synset('shoot.v.01')\n"
     ]
    }
   ],
   "source": [
    "# example sentence\n",
    "sentence = 'It was a musical hit'\n",
    "words = sentence.split()\n",
    "\n",
    "print(lesk(words, 'hit', 'n'))\n",
    "print(lesk(words, 'hit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('hit.n.05')\n",
      "Synset('shoot.v.01')\n"
     ]
    }
   ],
   "source": [
    "# example sentence\n",
    "sentence = 'Can I take a hit of that'\n",
    "words = sentence.split()\n",
    "\n",
    "print(lesk(words, 'hit', 'n'))\n",
    "print(lesk(words, 'hit'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the right synset from which a context-definition sentence's originates will be easier if the pos is specificied. Although, from my observation of the algorithm, you can notice that the algorithm mistakenly produced a synset with a definition that has nothing to do with my sentence even with the aid of the pos tagging. So the algorithm isn't always correct with the pos tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet\n",
    "SentiWordNet was designed for opinion mining, meaning that it was used to provide sentiment scores for positivitiy, negativity, and objectivity for a selected synset.\n",
    "\n",
    "Each value is always between 0 and 1, and the sum of the three scores is 1.0.\n",
    "\n",
    "I chose an emotionally charged word and the code below gives the polarity scores for each of the synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<expectation.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score:  0.0\n",
      "Negative Score:  0.0\n",
      "Objective Score:  1.0 \n",
      "\n",
      "<anticipation.n.04: PosScore=0.5 NegScore=0.0>\n",
      "Positive Score:  0.5\n",
      "Negative Score:  0.0\n",
      "Objective Score:  0.5 \n",
      "\n",
      "<expectation.n.03: PosScore=0.0 NegScore=0.125>\n",
      "Positive Score:  0.0\n",
      "Negative Score:  0.125\n",
      "Objective Score:  0.875 \n",
      "\n",
      "<arithmetic_mean.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score:  0.0\n",
      "Negative Score:  0.0\n",
      "Objective Score:  1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# an emotionally charged word\n",
    "expect = swn.senti_synsets('expectation')\n",
    "\n",
    "# find its senti-synsets and output the polarity scores for each word\n",
    "for ss in list(expect):\n",
    "    expect = ss\n",
    "    print(expect)\n",
    "    print('Positive Score: ', expect.pos_score())\n",
    "    print('Negative Score: ', expect.neg_score())\n",
    "    print('Objective Score: ', expect.obj_score(), '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to get the polarity score of each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['expecting', 'to', 'have', 'a', 'delivery', 'today']\n",
      "<expect.v.01: PosScore=0.25 NegScore=0.25>\n",
      "Positive Score:  0.25\n",
      "Negative Score:  0.25\n",
      "Objective Score:  0.5 \n",
      "\n",
      "<rich_person.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score:  0.0\n",
      "Negative Score:  0.0\n",
      "Objective Score:  1.0 \n",
      "\n",
      "<angstrom.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score:  0.0\n",
      "Negative Score:  0.0\n",
      "Objective Score:  1.0 \n",
      "\n",
      "<delivery.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive Score:  0.0\n",
      "Negative Score:  0.0\n",
      "Objective Score:  1.0 \n",
      "\n",
      "<today.n.01: PosScore=0.125 NegScore=0.0>\n",
      "Positive Score:  0.125\n",
      "Negative Score:  0.0\n",
      "Objective Score:  0.875 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# variables\n",
    "sentence = 'expecting to have a delivery today'\n",
    "neg = 0\n",
    "pos = 0\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)\n",
    "# output polarity for each word in the sentence\n",
    "for w in words:\n",
    "    ss_list = list(swn.senti_synsets(w))\n",
    "\n",
    "    if ss_list:\n",
    "        syn = ss_list[0] \n",
    "        print(syn)\n",
    "        print('Positive Score: ', syn.pos_score())\n",
    "        print('Negative Score: ', syn.neg_score())\n",
    "        print('Objective Score: ', syn.obj_score(), '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation of the scores and utility of knowing these scores in an NLP application, I have noticed that each key words returns their polarity scores. Also, stopwords are not taken into account because SentiWordNet does not classify them, so they are ignored. Future NLP applications that needs a program to analyze the sentiment behind a text will significantly benefit from using this. For an example, a program that may need to know if the user feels happy about the topic or find them dissatisfied with it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation\n",
    "Collocation is the natural juxtaposition of two or more words to create a deeper meaning than the simple coincidence of their placement. For example, the collocation 'gap year' means more than the individual words can express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "# output collocations for text4, Inaugural Corpus\n",
    "from nltk.book import text4\n",
    "text4.collocations()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's select one of the collocations identified by NLTK and calculate mutual information\n",
    "\n",
    "In order to calculate, we need to know that mutual information is the log of the probablity:\n",
    "\n",
    "P(x,y) / [P(x) * P(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p('fellow'):  0.013665835411471322\n",
      "p('citizens'):  0.026932668329177057\n",
      "p('fellow citizens'):  0.006084788029925187\n",
      "Mutual Information Score:  4.0472042737811735\n"
     ]
    }
   ],
   "source": [
    "# calculate mutual information\n",
    "import math\n",
    "\n",
    "# txt\n",
    "txt = ' '.join(text4.tokens)\n",
    "\n",
    "vocab = len(set(text4))\n",
    "\n",
    "# words\n",
    "fellow = txt.count('fellow') / vocab\n",
    "print('p(\\'fellow\\'): ', fellow)\n",
    "\n",
    "citizens = txt.count('citizens') / vocab\n",
    "print('p(\\'citizens\\'): ', citizens)\n",
    "\n",
    "fellow_citizens = txt.count('fellow citizens') / vocab\n",
    "print('p(\\'fellow citizens\\'): ', fellow_citizens)\n",
    "\n",
    "# calculate\n",
    "pmi = math.log2(fellow_citizens / (fellow * citizens))\n",
    "\n",
    "print('Mutual Information Score: ', pmi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Commentary on the Results of the Mutual Information Formula and my Interpretation\n",
    "\n",
    "The level of non-randomness that exists when the two words appear in text is determined by the number that is output during the MI score calculation. Mutual information is essential in determining how significant collocation is in a particular text and suggests that the target words might be attracted to one another in both directions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8c3ccee6358082b56d4bf6497bbfb0bbb5ad3534c94c8a9fbdfa83e71dff5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
